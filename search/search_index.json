{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home python library to interact with local or remote nifti medical images. Setup pip install -e . Dir structure For a given dataset, it assumes that the folder structure is: images: <dataset_name>/images/native/ labelmaps (segmentations): <dataset_name>/labelmaps/native/ For example, a dataset of Lung CT images would have a directory structure at <root_path>/lung : images/ |_ native/ |_ lung001.nii.gz |_ lung002.nii.gz |_ ... |_ subsampled1mm/ |_ lung001.nii.gz |_ lung002.nii.gz |_ ... labelmaps/ |_ l1 |_ native |_ lung001_seg.nii.gz |_ lung002_seg.nii.gz |_ ... |_ subsampled1mm |_ lung001_seg.nii.gz |_ lung002_seg.nii.gz |_ ... To match a labelmap/segmentation with its associated image, the image and its labelmap need to have the same prefix. Images that haven't been resampled should be placed in the native folder. Any resampled images should go in the corresponding folders, e.g. 1mm isotropic: subsampled1mm , 1mm in-plane isotropic and 4mm slice thickness: subsampled1-1-4mm Example Using local dataset from midatasets import MIDataset from midatasets.clients import LocalDatasetClient client = LocalDatasetClient ( root_dir = \"/data/datasets\" ) dataset = MIDataset ( dataset_id = \"lung\" , client = client ) for images in dataset . iterate_keys ( keys = [ \"image\" , \"labelmap/l1\" ], spacing = 0 ): print ( images ) Using dataset on S3 from midatasets import MIDataset from midatasets.clients import S3DatasetClient client = S3DatasetClient ( bucket = \"test\" , prefix = \"datasets\" ) dataset = MIDataset ( dataset_id = \"lung\" , client = client ) dataset . download () ## or download specific keys dataset . download ( keys = [ \"image\" ]) for images in dataset . iterate_keys ( keys = [ \"image\" , \"labelmap/l1\" ], spacing = 0 ): print ( images ) ## or download specific image images [ \"image\" ] . download ()","title":"Home"},{"location":"#home","text":"python library to interact with local or remote nifti medical images.","title":"Home"},{"location":"#setup","text":"pip install -e .","title":"Setup"},{"location":"#dir-structure","text":"For a given dataset, it assumes that the folder structure is: images: <dataset_name>/images/native/ labelmaps (segmentations): <dataset_name>/labelmaps/native/ For example, a dataset of Lung CT images would have a directory structure at <root_path>/lung : images/ |_ native/ |_ lung001.nii.gz |_ lung002.nii.gz |_ ... |_ subsampled1mm/ |_ lung001.nii.gz |_ lung002.nii.gz |_ ... labelmaps/ |_ l1 |_ native |_ lung001_seg.nii.gz |_ lung002_seg.nii.gz |_ ... |_ subsampled1mm |_ lung001_seg.nii.gz |_ lung002_seg.nii.gz |_ ... To match a labelmap/segmentation with its associated image, the image and its labelmap need to have the same prefix. Images that haven't been resampled should be placed in the native folder. Any resampled images should go in the corresponding folders, e.g. 1mm isotropic: subsampled1mm , 1mm in-plane isotropic and 4mm slice thickness: subsampled1-1-4mm","title":"Dir structure"},{"location":"#example","text":"","title":"Example"},{"location":"#using-local-dataset","text":"from midatasets import MIDataset from midatasets.clients import LocalDatasetClient client = LocalDatasetClient ( root_dir = \"/data/datasets\" ) dataset = MIDataset ( dataset_id = \"lung\" , client = client ) for images in dataset . iterate_keys ( keys = [ \"image\" , \"labelmap/l1\" ], spacing = 0 ): print ( images )","title":"Using local dataset"},{"location":"#using-dataset-on-s3","text":"from midatasets import MIDataset from midatasets.clients import S3DatasetClient client = S3DatasetClient ( bucket = \"test\" , prefix = \"datasets\" ) dataset = MIDataset ( dataset_id = \"lung\" , client = client ) dataset . download () ## or download specific keys dataset . download ( keys = [ \"image\" ]) for images in dataset . iterate_keys ( keys = [ \"image\" , \"labelmap/l1\" ], spacing = 0 ): print ( images ) ## or download specific image images [ \"image\" ] . download ()","title":"Using dataset on S3"},{"location":"references/MIDataset/","text":"Source code in midatasets/midataset.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 class MIDataset : def __init__ ( self , dataset_id : Union [ int , str ], client : DatasetClientBase , base_dir : str = \"/tmp\" , default_bucket : str = \"local\" ): \"\"\" Dataset class Args: dataset_id: dataset id client: client base_dir: base dir default_bucket: default s3 bucket \"\"\" self . client = client self . dataset_id = dataset_id self . _images = None self . _info = None self . base_dir = base_dir self . bucket = default_bucket @property def info ( self ) -> Dataset : if self . _info is None : self . _info = self . client . get_dataset ( self . dataset_id ) return self . _info @property def dataset_base_dir ( self ): return f \" { self . base_dir } /datasets/ { self . dataset_id } \" def get_labels ( self , key : str ): return self . info . label_mappings . get ( key , None ) @property def images ( self ) -> List [ Image ]: if self . _images is None : self . _images = self . client . get_images ( self . dataset_id ) return self . _images def download ( self , keys = ( \"image\" ,), overwrite = False ): for images in self . iterate_keys ( keys = keys ): for k , image in images . items (): image . download ( overwrite = overwrite ) def get_resampled_mimage ( self , image : MImage , target_spacing ): prefix = image . prefix . replace ( image . resolution_dir , get_spacing_dirname ( target_spacing ) ) local_path = image . local_path . replace ( image . resolution_dir , get_spacing_dirname ( target_spacing ) ) return MImage ( bucket = image . bucket , prefix = prefix , key = image . key , base_dir = image . base_dir , validate_key = False , local_path = local_path , ) def _create_mimage ( self , path : str , key : str , name : str ): if path . startswith ( \"s3://\" ): return MImage . from_s3_path ( path , base_dir = self . base_dir , validate_key = False , key = key , local_path = f \" { self . dataset_base_dir } / { get_key_dirname ( key ) } / { get_spacing_dirname ( 0 ) } / { name } .nii.gz\" , ) else : return MImage . from_local_path ( base_dir = self . base_dir , validate_key = False , key = key , local_path = path , bucket = self . bucket ) def iterate_key ( self , key : str , spacing : Union [ float , int ] = 0 ): \"\"\" iterate over objects with a given key Args: key: the key to iterate over spacing: Returns: Examples: Iterate over keys to download images >>> dataset = MIDataset(dataset_id=\"1\") >>> for obj in dataset.iterate_key(key=\"image\"): >>> obj.download() \"\"\" remap_keys = self . info . label_mappings . get ( \"_remap_keys\" , {}) for image in self . images : for artifact in image . artifacts : if key == remap_keys . get ( artifact . key , artifact . key ) and artifact . path . endswith ( \".nii.gz\" ): obj = self . _create_mimage ( artifact . path , key = key , name = image . name ) if spacing != 0 : obj = self . get_resampled_mimage ( obj , target_spacing = spacing ) yield obj def iterate_keys ( self , keys = ( \"image\" ,), spacing : Union [ float , int ] = 0 , allow_missing : bool = False ) -> Dict [ str , MImage ]: \"\"\" iterate over multiple keys Args: keys: the list of keys to iterate over spacing: the specific spacing of the images allow_missing: if True then also includes the images that don't have all the keys Returns: Examples: Iterate over keys to download images >>> dataset = MIDataset(dataset_id=\"1\") >>> for artifacts in dataset.iterate_keys(keys=[\"image\", \"labelmap/lungmask\"]): >>> for k, obj in artifacts.items(): >>> obj.download() \"\"\" remap_keys = self . info . label_mappings . get ( \"_remap_keys\" , {}) for image in self . images : artifacts = {} for artifact in image . artifacts : key = remap_keys . get ( artifact . key , artifact . key ) if key in keys and artifact . path . endswith ( \".nii.gz\" ): artifacts [ key ] = self . _create_mimage ( artifact . path , key = key , name = image . name ) if spacing != 0 : artifacts [ key ] = self . get_resampled_mimage ( artifacts [ key ], target_spacing = spacing ) if allow_missing or len ( artifacts ) == len ( keys ): yield artifacts def resample ( self , target_spacing , keys = ( \"image\" ,)): \"\"\" resample images to a specific spacing Args: target_spacing: the target spacing to resample to keys: the keys to resample Returns: Examples: Download from s3 and resample >>> dataset = MIDataset(dataset_id=\"lung\") >>> dataset.download(keys=[\"image\", \"labelmap\"]) >>> dataset.resample(target_spacing=1, keys=[\"image\"]) \"\"\" images = [] for image in self . iterate_keys ( keys = keys , spacing = 0 ): images . extend ( image . values ()) resample_mimage_parallel ( images , target_spacing = target_spacing ) def get_dir ( self , key : str , spacing : Union [ int , float ] = 0 ) -> Path : \"\"\" Get the directory of a given key at a given spacing Args: key: the key of the image spacing: the spacing Returns: \"\"\" return Path ( f \" { self . dataset_base_dir } / { get_key_dirname ( key ) } / { get_spacing_dirname ( spacing ) } \" ) __init__ ( dataset_id , client , base_dir = '/tmp' , default_bucket = 'local' ) Dataset class Parameters: Name Type Description Default dataset_id Union [ int , str ] dataset id required client DatasetClientBase client required base_dir str base dir '/tmp' default_bucket str default s3 bucket 'local' Source code in midatasets/midataset.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , dataset_id : Union [ int , str ], client : DatasetClientBase , base_dir : str = \"/tmp\" , default_bucket : str = \"local\" ): \"\"\" Dataset class Args: dataset_id: dataset id client: client base_dir: base dir default_bucket: default s3 bucket \"\"\" self . client = client self . dataset_id = dataset_id self . _images = None self . _info = None self . base_dir = base_dir self . bucket = default_bucket get_dir ( key , spacing = 0 ) Get the directory of a given key at a given spacing Parameters: Name Type Description Default key str the key of the image required spacing Union [ int , float ] the spacing 0 Source code in midatasets/midataset.py 177 178 179 180 181 182 183 184 185 186 187 def get_dir ( self , key : str , spacing : Union [ int , float ] = 0 ) -> Path : \"\"\" Get the directory of a given key at a given spacing Args: key: the key of the image spacing: the spacing Returns: \"\"\" return Path ( f \" { self . dataset_base_dir } / { get_key_dirname ( key ) } / { get_spacing_dirname ( spacing ) } \" ) iterate_key ( key , spacing = 0 ) iterate over objects with a given key Parameters: Name Type Description Default key str the key to iterate over required spacing Union [ float , int ] 0 Examples: Iterate over keys to download images >>> dataset = MIDataset ( dataset_id = \"1\" ) >>> for obj in dataset . iterate_key ( key = \"image\" ): >>> obj . download () Source code in midatasets/midataset.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def iterate_key ( self , key : str , spacing : Union [ float , int ] = 0 ): \"\"\" iterate over objects with a given key Args: key: the key to iterate over spacing: Returns: Examples: Iterate over keys to download images >>> dataset = MIDataset(dataset_id=\"1\") >>> for obj in dataset.iterate_key(key=\"image\"): >>> obj.download() \"\"\" remap_keys = self . info . label_mappings . get ( \"_remap_keys\" , {}) for image in self . images : for artifact in image . artifacts : if key == remap_keys . get ( artifact . key , artifact . key ) and artifact . path . endswith ( \".nii.gz\" ): obj = self . _create_mimage ( artifact . path , key = key , name = image . name ) if spacing != 0 : obj = self . get_resampled_mimage ( obj , target_spacing = spacing ) yield obj iterate_keys ( keys = ( 'image' ), spacing = 0 , allow_missing = False ) iterate over multiple keys Parameters: Name Type Description Default keys the list of keys to iterate over ('image') spacing Union [ float , int ] the specific spacing of the images 0 allow_missing bool if True then also includes the images that don't have all the keys False Examples: Iterate over keys to download images >>> dataset = MIDataset ( dataset_id = \"1\" ) >>> for artifacts in dataset . iterate_keys ( keys = [ \"image\" , \"labelmap/lungmask\" ]): >>> for k , obj in artifacts . items (): >>> obj . download () Source code in midatasets/midataset.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def iterate_keys ( self , keys = ( \"image\" ,), spacing : Union [ float , int ] = 0 , allow_missing : bool = False ) -> Dict [ str , MImage ]: \"\"\" iterate over multiple keys Args: keys: the list of keys to iterate over spacing: the specific spacing of the images allow_missing: if True then also includes the images that don't have all the keys Returns: Examples: Iterate over keys to download images >>> dataset = MIDataset(dataset_id=\"1\") >>> for artifacts in dataset.iterate_keys(keys=[\"image\", \"labelmap/lungmask\"]): >>> for k, obj in artifacts.items(): >>> obj.download() \"\"\" remap_keys = self . info . label_mappings . get ( \"_remap_keys\" , {}) for image in self . images : artifacts = {} for artifact in image . artifacts : key = remap_keys . get ( artifact . key , artifact . key ) if key in keys and artifact . path . endswith ( \".nii.gz\" ): artifacts [ key ] = self . _create_mimage ( artifact . path , key = key , name = image . name ) if spacing != 0 : artifacts [ key ] = self . get_resampled_mimage ( artifacts [ key ], target_spacing = spacing ) if allow_missing or len ( artifacts ) == len ( keys ): yield artifacts resample ( target_spacing , keys = ( 'image' )) resample images to a specific spacing Parameters: Name Type Description Default target_spacing the target spacing to resample to required keys the keys to resample ('image') Examples: Download from s3 and resample >>> dataset = MIDataset ( dataset_id = \"lung\" ) >>> dataset . download ( keys = [ \"image\" , \"labelmap\" ]) >>> dataset . resample ( target_spacing = 1 , keys = [ \"image\" ]) Source code in midatasets/midataset.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def resample ( self , target_spacing , keys = ( \"image\" ,)): \"\"\" resample images to a specific spacing Args: target_spacing: the target spacing to resample to keys: the keys to resample Returns: Examples: Download from s3 and resample >>> dataset = MIDataset(dataset_id=\"lung\") >>> dataset.download(keys=[\"image\", \"labelmap\"]) >>> dataset.resample(target_spacing=1, keys=[\"image\"]) \"\"\" images = [] for image in self . iterate_keys ( keys = keys , spacing = 0 ): images . extend ( image . values ()) resample_mimage_parallel ( images , target_spacing = target_spacing )","title":"MIDataset"},{"location":"references/MIDataset/#midatasets.midataset.MIDataset.__init__","text":"Dataset class Parameters: Name Type Description Default dataset_id Union [ int , str ] dataset id required client DatasetClientBase client required base_dir str base dir '/tmp' default_bucket str default s3 bucket 'local' Source code in midatasets/midataset.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 def __init__ ( self , dataset_id : Union [ int , str ], client : DatasetClientBase , base_dir : str = \"/tmp\" , default_bucket : str = \"local\" ): \"\"\" Dataset class Args: dataset_id: dataset id client: client base_dir: base dir default_bucket: default s3 bucket \"\"\" self . client = client self . dataset_id = dataset_id self . _images = None self . _info = None self . base_dir = base_dir self . bucket = default_bucket","title":"__init__()"},{"location":"references/MIDataset/#midatasets.midataset.MIDataset.get_dir","text":"Get the directory of a given key at a given spacing Parameters: Name Type Description Default key str the key of the image required spacing Union [ int , float ] the spacing 0 Source code in midatasets/midataset.py 177 178 179 180 181 182 183 184 185 186 187 def get_dir ( self , key : str , spacing : Union [ int , float ] = 0 ) -> Path : \"\"\" Get the directory of a given key at a given spacing Args: key: the key of the image spacing: the spacing Returns: \"\"\" return Path ( f \" { self . dataset_base_dir } / { get_key_dirname ( key ) } / { get_spacing_dirname ( spacing ) } \" )","title":"get_dir()"},{"location":"references/MIDataset/#midatasets.midataset.MIDataset.iterate_key","text":"iterate over objects with a given key Parameters: Name Type Description Default key str the key to iterate over required spacing Union [ float , int ] 0 Examples: Iterate over keys to download images >>> dataset = MIDataset ( dataset_id = \"1\" ) >>> for obj in dataset . iterate_key ( key = \"image\" ): >>> obj . download () Source code in midatasets/midataset.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def iterate_key ( self , key : str , spacing : Union [ float , int ] = 0 ): \"\"\" iterate over objects with a given key Args: key: the key to iterate over spacing: Returns: Examples: Iterate over keys to download images >>> dataset = MIDataset(dataset_id=\"1\") >>> for obj in dataset.iterate_key(key=\"image\"): >>> obj.download() \"\"\" remap_keys = self . info . label_mappings . get ( \"_remap_keys\" , {}) for image in self . images : for artifact in image . artifacts : if key == remap_keys . get ( artifact . key , artifact . key ) and artifact . path . endswith ( \".nii.gz\" ): obj = self . _create_mimage ( artifact . path , key = key , name = image . name ) if spacing != 0 : obj = self . get_resampled_mimage ( obj , target_spacing = spacing ) yield obj","title":"iterate_key()"},{"location":"references/MIDataset/#midatasets.midataset.MIDataset.iterate_keys","text":"iterate over multiple keys Parameters: Name Type Description Default keys the list of keys to iterate over ('image') spacing Union [ float , int ] the specific spacing of the images 0 allow_missing bool if True then also includes the images that don't have all the keys False Examples: Iterate over keys to download images >>> dataset = MIDataset ( dataset_id = \"1\" ) >>> for artifacts in dataset . iterate_keys ( keys = [ \"image\" , \"labelmap/lungmask\" ]): >>> for k , obj in artifacts . items (): >>> obj . download () Source code in midatasets/midataset.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 def iterate_keys ( self , keys = ( \"image\" ,), spacing : Union [ float , int ] = 0 , allow_missing : bool = False ) -> Dict [ str , MImage ]: \"\"\" iterate over multiple keys Args: keys: the list of keys to iterate over spacing: the specific spacing of the images allow_missing: if True then also includes the images that don't have all the keys Returns: Examples: Iterate over keys to download images >>> dataset = MIDataset(dataset_id=\"1\") >>> for artifacts in dataset.iterate_keys(keys=[\"image\", \"labelmap/lungmask\"]): >>> for k, obj in artifacts.items(): >>> obj.download() \"\"\" remap_keys = self . info . label_mappings . get ( \"_remap_keys\" , {}) for image in self . images : artifacts = {} for artifact in image . artifacts : key = remap_keys . get ( artifact . key , artifact . key ) if key in keys and artifact . path . endswith ( \".nii.gz\" ): artifacts [ key ] = self . _create_mimage ( artifact . path , key = key , name = image . name ) if spacing != 0 : artifacts [ key ] = self . get_resampled_mimage ( artifacts [ key ], target_spacing = spacing ) if allow_missing or len ( artifacts ) == len ( keys ): yield artifacts","title":"iterate_keys()"},{"location":"references/MIDataset/#midatasets.midataset.MIDataset.resample","text":"resample images to a specific spacing Parameters: Name Type Description Default target_spacing the target spacing to resample to required keys the keys to resample ('image') Examples: Download from s3 and resample >>> dataset = MIDataset ( dataset_id = \"lung\" ) >>> dataset . download ( keys = [ \"image\" , \"labelmap\" ]) >>> dataset . resample ( target_spacing = 1 , keys = [ \"image\" ]) Source code in midatasets/midataset.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def resample ( self , target_spacing , keys = ( \"image\" ,)): \"\"\" resample images to a specific spacing Args: target_spacing: the target spacing to resample to keys: the keys to resample Returns: Examples: Download from s3 and resample >>> dataset = MIDataset(dataset_id=\"lung\") >>> dataset.download(keys=[\"image\", \"labelmap\"]) >>> dataset.resample(target_spacing=1, keys=[\"image\"]) \"\"\" images = [] for image in self . iterate_keys ( keys = keys , spacing = 0 ): images . extend ( image . values ()) resample_mimage_parallel ( images , target_spacing = target_spacing )","title":"resample()"},{"location":"references/MImage/","text":"MImage Bases: MObject MImage added functionality for loading image metadata if available Source code in midatasets/mimage.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 class MImage ( MObject ): \"\"\" MImage added functionality for loading image metadata if available \"\"\" def __init__ ( self , bucket : str , prefix : str , key : str , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , validate_key : bool = False , ): super () . __init__ ( bucket = bucket , prefix = prefix , key = key , local_path = local_path , base_dir = base_dir , validate_key = validate_key , ) self . _shape = None self . _affine = None def _load_metadata ( self ): native_img = nib . load ( self . local_path ) self . _shape = native_img . shape self . _affine = native_img . affine del native_img @property def shape ( self ): if self . _shape is None : self . _load_metadata () return self . _shape @property def affine ( self ): \"\"\" Returns: affine metadata \"\"\" if self . _affine is None : self . _load_metadata () return self . _affine @property def resolution_dir ( self ): return Path ( self . prefix ) . parent . name affine property MObject Bases: S3Object Source code in midatasets/mimage.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 class MObject ( S3Object ): def __init__ ( self , bucket : str , prefix : str , key : str , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , validate_key : bool = True , ): \"\"\" Args: bucket: s3 bucket prefix: s3 prefix key: a key that represents the objects local_path: a specific local path to use. If None it is derived from prefix and base_dir base_dir: a base directory to use locally validate_key: whether to check if the key is part of the prefix \"\"\" super () . __init__ ( bucket = bucket , prefix = prefix , local_path = local_path , base_dir = base_dir , key = key , ) if validate_key : self . validate () def validate ( self ): if self . key_dir not in self . prefix : raise KeyError ( f \"` { self . key_dir } ` not part of ` { self . prefix } `\" ) @property def key_dir ( self ): \"\"\" Returns: the key directory \"\"\" return get_key_dirname ( self . key ) @property def base_prefix ( self ): if self . key_dir in self . prefix : return self . prefix . split ( f \"/ { self . key_dir } \" )[ 0 ] else : return self . prefix . rsplit ( \"/\" , 1 )[ 0 ] @property def subprefix ( self ): return str ( Path ( self . prefix ) . relative_to ( self . base_prefix )) def derive ( self , new_key : str , local_path : Optional [ str ] = None , prefix : Optional [ str ] = None ): \"\"\" Derive an object with a new key :param new_key: :param local_path: :return: \"\"\" if prefix is None : if self . key_dir not in self . prefix : raise Exception ( f \" { self . key_dir } not part of { self . prefix } \" ) prefix = self . prefix . replace ( self . key_dir , get_key_dirname ( new_key )) if local_path is not None : prefix = prefix . replace ( self . extension , get_extension ( local_path )) return self . __class__ ( bucket = self . bucket , prefix = prefix , key = new_key , base_dir = self . base_dir , validate_key = False , local_path = local_path , ) key_dir property __init__ ( bucket , prefix , key , local_path = None , base_dir = '/tmp' , validate_key = True ) Parameters: Name Type Description Default bucket str s3 bucket required prefix str s3 prefix required key str a key that represents the objects required local_path Optional [ str ] a specific local path to use. If None it is derived from prefix and base_dir None base_dir str a base directory to use locally '/tmp' validate_key bool whether to check if the key is part of the prefix True Source code in midatasets/mimage.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def __init__ ( self , bucket : str , prefix : str , key : str , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , validate_key : bool = True , ): \"\"\" Args: bucket: s3 bucket prefix: s3 prefix key: a key that represents the objects local_path: a specific local path to use. If None it is derived from prefix and base_dir base_dir: a base directory to use locally validate_key: whether to check if the key is part of the prefix \"\"\" super () . __init__ ( bucket = bucket , prefix = prefix , local_path = local_path , base_dir = base_dir , key = key , ) if validate_key : self . validate () derive ( new_key , local_path = None , prefix = None ) Derive an object with a new key :param new_key: :param local_path: :return: Source code in midatasets/mimage.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def derive ( self , new_key : str , local_path : Optional [ str ] = None , prefix : Optional [ str ] = None ): \"\"\" Derive an object with a new key :param new_key: :param local_path: :return: \"\"\" if prefix is None : if self . key_dir not in self . prefix : raise Exception ( f \" { self . key_dir } not part of { self . prefix } \" ) prefix = self . prefix . replace ( self . key_dir , get_key_dirname ( new_key )) if local_path is not None : prefix = prefix . replace ( self . extension , get_extension ( local_path )) return self . __class__ ( bucket = self . bucket , prefix = prefix , key = new_key , base_dir = self . base_dir , validate_key = False , local_path = local_path , ) S3Object A class to represent an s3 object to make it easier to upload and download Source code in midatasets/mimage.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class S3Object : \"\"\" A class to represent an s3 object to make it easier to upload and download \"\"\" def __init__ ( self , bucket : str , prefix : str , key : Optional [ str ] = None , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , s3_client : Optional [ S3Boto3 ] = None , ** kwargs , ): \"\"\" Args: bucket: s3 bucket prefix: s3 prefix key: a key that represents the objects local_path: a specific local path to use. If None it is derived from prefix and base_dir base_dir: a base directory to use locally s3_client: the s3 client **kwargs: \"\"\" self . bucket = bucket self . prefix = prefix self . base_dir = base_dir self . key = key self . _name = None self . _ext = None self . _local_path = local_path self . s3_client = s3_client or S3Boto3 () def __repr__ ( self ): return f \" { self . __class__ . __name__ } (name= { self . name } , s3_path= { self . s3_path } , local_path= { self . local_path } )\" @classmethod def from_s3_path ( cls , s3_path : str , base_dir : str = \"/tmp\" , key : Optional [ str ] = None , local_path : Optional [ str ] = None , ** kwargs , ): path_parts = s3_path . replace ( \"s3://\" , \"\" ) . split ( \"/\" ) bucket = path_parts . pop ( 0 ) prefix = \"/\" . join ( path_parts ) return cls ( bucket = bucket , prefix = prefix , base_dir = base_dir , key = key , local_path = local_path , ** kwargs , ) @classmethod def from_local_path ( cls , local_path : str , base_dir : str = \"/tmp\" , key : Optional [ str ] = None , ** kwargs , ): if base_dir not in local_path : raise Exception ( f \"base_dir { base_dir } not part of { local_path } \" ) return cls ( bucket = kwargs . pop ( \"bucket\" ) if \"bucket\" in kwargs else \"local\" , prefix = kwargs . pop ( \"prefix\" ) if \"prefix\" in kwargs else str ( Path ( local_path ) . relative_to ( base_dir )), base_dir = base_dir , key = key , local_path = local_path , ** kwargs , ) @property def local_path ( self ): \"\"\" use provided local path; otherwise, use from prefix \"\"\" return self . _local_path or str ( Path ( f \" { self . base_dir } / { self . prefix } \" )) @property def s3_path ( self ) -> str : \"\"\" Returns: full s3 path \"\"\" return f \"s3:// { self . bucket } / { self . prefix } \" @property def extension ( self ) -> str : \"\"\" Returns: file extension \"\"\" if self . _ext is None : self . _ext = get_extension ( self . prefix ) return self . _ext @property def name ( self ): \"\"\" Returns: name of object \"\"\" if self . _name is None : path = Path ( self . prefix ) self . _name = path . name . replace ( self . extension , \"\" ) return self . _name @property def basename ( self ): \"\"\" Returns: basename from prefix \"\"\" return Path ( self . prefix ) . name def download ( self , overwrite : bool = False ): \"\"\" Download an object from s3 Args: overwrite: Whether to overwrite if it already exists Returns: None \"\"\" self . s3_client . download_file ( bucket = self . bucket , prefix = self . prefix , target = self . local_path , overwrite = overwrite ) def upload ( self , overwrite : bool = False ) -> None : \"\"\" Upload an object to s3 Args: overwrite: Whether to overwrite if it already exists Returns: None \"\"\" self . s3_client . upload_file ( file_name = self . local_path , bucket = self . bucket , prefix = self . prefix , overwrite = overwrite ) def exists_local ( self ): \"\"\" Check if object exists locally Returns: True if exists \"\"\" return os . path . exists ( self . local_path ) def exists_remote ( self ): \"\"\" Check if object exists on s3 Returns: True if exists \"\"\" return self . s3_client . check_exists ( self . bucket , self . prefix ) def delete ( self ): \"\"\" Delete an object locally Returns: \"\"\" try : os . remove ( self . local_path ) logger . info ( f \"[Removed] { self . local_path } \" ) except Exception as e : logger . warning ( e ) basename property extension : str property local_path property use provided local path; otherwise, use from prefix name property s3_path : str property __init__ ( bucket , prefix , key = None , local_path = None , base_dir = '/tmp' , s3_client = None , ** kwargs ) Parameters: Name Type Description Default bucket str s3 bucket required prefix str s3 prefix required key Optional [ str ] a key that represents the objects None local_path Optional [ str ] a specific local path to use. If None it is derived from prefix and base_dir None base_dir str a base directory to use locally '/tmp' s3_client Optional [ S3Boto3 ] the s3 client None **kwargs {} Source code in midatasets/mimage.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , bucket : str , prefix : str , key : Optional [ str ] = None , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , s3_client : Optional [ S3Boto3 ] = None , ** kwargs , ): \"\"\" Args: bucket: s3 bucket prefix: s3 prefix key: a key that represents the objects local_path: a specific local path to use. If None it is derived from prefix and base_dir base_dir: a base directory to use locally s3_client: the s3 client **kwargs: \"\"\" self . bucket = bucket self . prefix = prefix self . base_dir = base_dir self . key = key self . _name = None self . _ext = None self . _local_path = local_path self . s3_client = s3_client or S3Boto3 () delete () Delete an object locally Source code in midatasets/mimage.py 179 180 181 182 183 184 185 186 187 188 189 def delete ( self ): \"\"\" Delete an object locally Returns: \"\"\" try : os . remove ( self . local_path ) logger . info ( f \"[Removed] { self . local_path } \" ) except Exception as e : logger . warning ( e ) download ( overwrite = False ) Download an object from s3 Parameters: Name Type Description Default overwrite bool Whether to overwrite if it already exists False Source code in midatasets/mimage.py 138 139 140 141 142 143 144 145 146 147 148 149 150 def download ( self , overwrite : bool = False ): \"\"\" Download an object from s3 Args: overwrite: Whether to overwrite if it already exists Returns: None \"\"\" self . s3_client . download_file ( bucket = self . bucket , prefix = self . prefix , target = self . local_path , overwrite = overwrite ) exists_local () Check if object exists locally Source code in midatasets/mimage.py 163 164 165 166 167 168 169 def exists_local ( self ): \"\"\" Check if object exists locally Returns: True if exists \"\"\" return os . path . exists ( self . local_path ) exists_remote () Check if object exists on s3 Source code in midatasets/mimage.py 171 172 173 174 175 176 177 def exists_remote ( self ): \"\"\" Check if object exists on s3 Returns: True if exists \"\"\" return self . s3_client . check_exists ( self . bucket , self . prefix ) upload ( overwrite = False ) Upload an object to s3 Parameters: Name Type Description Default overwrite bool Whether to overwrite if it already exists False Source code in midatasets/mimage.py 152 153 154 155 156 157 158 159 160 161 def upload ( self , overwrite : bool = False ) -> None : \"\"\" Upload an object to s3 Args: overwrite: Whether to overwrite if it already exists Returns: None \"\"\" self . s3_client . upload_file ( file_name = self . local_path , bucket = self . bucket , prefix = self . prefix , overwrite = overwrite )","title":"MImage"},{"location":"references/MImage/#midatasets.mimage.MImage","text":"Bases: MObject MImage added functionality for loading image metadata if available Source code in midatasets/mimage.py 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 class MImage ( MObject ): \"\"\" MImage added functionality for loading image metadata if available \"\"\" def __init__ ( self , bucket : str , prefix : str , key : str , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , validate_key : bool = False , ): super () . __init__ ( bucket = bucket , prefix = prefix , key = key , local_path = local_path , base_dir = base_dir , validate_key = validate_key , ) self . _shape = None self . _affine = None def _load_metadata ( self ): native_img = nib . load ( self . local_path ) self . _shape = native_img . shape self . _affine = native_img . affine del native_img @property def shape ( self ): if self . _shape is None : self . _load_metadata () return self . _shape @property def affine ( self ): \"\"\" Returns: affine metadata \"\"\" if self . _affine is None : self . _load_metadata () return self . _affine @property def resolution_dir ( self ): return Path ( self . prefix ) . parent . name","title":"MImage"},{"location":"references/MImage/#midatasets.mimage.MImage.affine","text":"","title":"affine"},{"location":"references/MImage/#midatasets.mimage.MObject","text":"Bases: S3Object Source code in midatasets/mimage.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 class MObject ( S3Object ): def __init__ ( self , bucket : str , prefix : str , key : str , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , validate_key : bool = True , ): \"\"\" Args: bucket: s3 bucket prefix: s3 prefix key: a key that represents the objects local_path: a specific local path to use. If None it is derived from prefix and base_dir base_dir: a base directory to use locally validate_key: whether to check if the key is part of the prefix \"\"\" super () . __init__ ( bucket = bucket , prefix = prefix , local_path = local_path , base_dir = base_dir , key = key , ) if validate_key : self . validate () def validate ( self ): if self . key_dir not in self . prefix : raise KeyError ( f \"` { self . key_dir } ` not part of ` { self . prefix } `\" ) @property def key_dir ( self ): \"\"\" Returns: the key directory \"\"\" return get_key_dirname ( self . key ) @property def base_prefix ( self ): if self . key_dir in self . prefix : return self . prefix . split ( f \"/ { self . key_dir } \" )[ 0 ] else : return self . prefix . rsplit ( \"/\" , 1 )[ 0 ] @property def subprefix ( self ): return str ( Path ( self . prefix ) . relative_to ( self . base_prefix )) def derive ( self , new_key : str , local_path : Optional [ str ] = None , prefix : Optional [ str ] = None ): \"\"\" Derive an object with a new key :param new_key: :param local_path: :return: \"\"\" if prefix is None : if self . key_dir not in self . prefix : raise Exception ( f \" { self . key_dir } not part of { self . prefix } \" ) prefix = self . prefix . replace ( self . key_dir , get_key_dirname ( new_key )) if local_path is not None : prefix = prefix . replace ( self . extension , get_extension ( local_path )) return self . __class__ ( bucket = self . bucket , prefix = prefix , key = new_key , base_dir = self . base_dir , validate_key = False , local_path = local_path , )","title":"MObject"},{"location":"references/MImage/#midatasets.mimage.MObject.key_dir","text":"","title":"key_dir"},{"location":"references/MImage/#midatasets.mimage.MObject.__init__","text":"Parameters: Name Type Description Default bucket str s3 bucket required prefix str s3 prefix required key str a key that represents the objects required local_path Optional [ str ] a specific local path to use. If None it is derived from prefix and base_dir None base_dir str a base directory to use locally '/tmp' validate_key bool whether to check if the key is part of the prefix True Source code in midatasets/mimage.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def __init__ ( self , bucket : str , prefix : str , key : str , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , validate_key : bool = True , ): \"\"\" Args: bucket: s3 bucket prefix: s3 prefix key: a key that represents the objects local_path: a specific local path to use. If None it is derived from prefix and base_dir base_dir: a base directory to use locally validate_key: whether to check if the key is part of the prefix \"\"\" super () . __init__ ( bucket = bucket , prefix = prefix , local_path = local_path , base_dir = base_dir , key = key , ) if validate_key : self . validate ()","title":"__init__()"},{"location":"references/MImage/#midatasets.mimage.MObject.derive","text":"Derive an object with a new key :param new_key: :param local_path: :return: Source code in midatasets/mimage.py 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 def derive ( self , new_key : str , local_path : Optional [ str ] = None , prefix : Optional [ str ] = None ): \"\"\" Derive an object with a new key :param new_key: :param local_path: :return: \"\"\" if prefix is None : if self . key_dir not in self . prefix : raise Exception ( f \" { self . key_dir } not part of { self . prefix } \" ) prefix = self . prefix . replace ( self . key_dir , get_key_dirname ( new_key )) if local_path is not None : prefix = prefix . replace ( self . extension , get_extension ( local_path )) return self . __class__ ( bucket = self . bucket , prefix = prefix , key = new_key , base_dir = self . base_dir , validate_key = False , local_path = local_path , )","title":"derive()"},{"location":"references/MImage/#midatasets.mimage.S3Object","text":"A class to represent an s3 object to make it easier to upload and download Source code in midatasets/mimage.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 class S3Object : \"\"\" A class to represent an s3 object to make it easier to upload and download \"\"\" def __init__ ( self , bucket : str , prefix : str , key : Optional [ str ] = None , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , s3_client : Optional [ S3Boto3 ] = None , ** kwargs , ): \"\"\" Args: bucket: s3 bucket prefix: s3 prefix key: a key that represents the objects local_path: a specific local path to use. If None it is derived from prefix and base_dir base_dir: a base directory to use locally s3_client: the s3 client **kwargs: \"\"\" self . bucket = bucket self . prefix = prefix self . base_dir = base_dir self . key = key self . _name = None self . _ext = None self . _local_path = local_path self . s3_client = s3_client or S3Boto3 () def __repr__ ( self ): return f \" { self . __class__ . __name__ } (name= { self . name } , s3_path= { self . s3_path } , local_path= { self . local_path } )\" @classmethod def from_s3_path ( cls , s3_path : str , base_dir : str = \"/tmp\" , key : Optional [ str ] = None , local_path : Optional [ str ] = None , ** kwargs , ): path_parts = s3_path . replace ( \"s3://\" , \"\" ) . split ( \"/\" ) bucket = path_parts . pop ( 0 ) prefix = \"/\" . join ( path_parts ) return cls ( bucket = bucket , prefix = prefix , base_dir = base_dir , key = key , local_path = local_path , ** kwargs , ) @classmethod def from_local_path ( cls , local_path : str , base_dir : str = \"/tmp\" , key : Optional [ str ] = None , ** kwargs , ): if base_dir not in local_path : raise Exception ( f \"base_dir { base_dir } not part of { local_path } \" ) return cls ( bucket = kwargs . pop ( \"bucket\" ) if \"bucket\" in kwargs else \"local\" , prefix = kwargs . pop ( \"prefix\" ) if \"prefix\" in kwargs else str ( Path ( local_path ) . relative_to ( base_dir )), base_dir = base_dir , key = key , local_path = local_path , ** kwargs , ) @property def local_path ( self ): \"\"\" use provided local path; otherwise, use from prefix \"\"\" return self . _local_path or str ( Path ( f \" { self . base_dir } / { self . prefix } \" )) @property def s3_path ( self ) -> str : \"\"\" Returns: full s3 path \"\"\" return f \"s3:// { self . bucket } / { self . prefix } \" @property def extension ( self ) -> str : \"\"\" Returns: file extension \"\"\" if self . _ext is None : self . _ext = get_extension ( self . prefix ) return self . _ext @property def name ( self ): \"\"\" Returns: name of object \"\"\" if self . _name is None : path = Path ( self . prefix ) self . _name = path . name . replace ( self . extension , \"\" ) return self . _name @property def basename ( self ): \"\"\" Returns: basename from prefix \"\"\" return Path ( self . prefix ) . name def download ( self , overwrite : bool = False ): \"\"\" Download an object from s3 Args: overwrite: Whether to overwrite if it already exists Returns: None \"\"\" self . s3_client . download_file ( bucket = self . bucket , prefix = self . prefix , target = self . local_path , overwrite = overwrite ) def upload ( self , overwrite : bool = False ) -> None : \"\"\" Upload an object to s3 Args: overwrite: Whether to overwrite if it already exists Returns: None \"\"\" self . s3_client . upload_file ( file_name = self . local_path , bucket = self . bucket , prefix = self . prefix , overwrite = overwrite ) def exists_local ( self ): \"\"\" Check if object exists locally Returns: True if exists \"\"\" return os . path . exists ( self . local_path ) def exists_remote ( self ): \"\"\" Check if object exists on s3 Returns: True if exists \"\"\" return self . s3_client . check_exists ( self . bucket , self . prefix ) def delete ( self ): \"\"\" Delete an object locally Returns: \"\"\" try : os . remove ( self . local_path ) logger . info ( f \"[Removed] { self . local_path } \" ) except Exception as e : logger . warning ( e )","title":"S3Object"},{"location":"references/MImage/#midatasets.mimage.S3Object.basename","text":"","title":"basename"},{"location":"references/MImage/#midatasets.mimage.S3Object.extension","text":"","title":"extension"},{"location":"references/MImage/#midatasets.mimage.S3Object.local_path","text":"use provided local path; otherwise, use from prefix","title":"local_path"},{"location":"references/MImage/#midatasets.mimage.S3Object.name","text":"","title":"name"},{"location":"references/MImage/#midatasets.mimage.S3Object.s3_path","text":"","title":"s3_path"},{"location":"references/MImage/#midatasets.mimage.S3Object.__init__","text":"Parameters: Name Type Description Default bucket str s3 bucket required prefix str s3 prefix required key Optional [ str ] a key that represents the objects None local_path Optional [ str ] a specific local path to use. If None it is derived from prefix and base_dir None base_dir str a base directory to use locally '/tmp' s3_client Optional [ S3Boto3 ] the s3 client None **kwargs {} Source code in midatasets/mimage.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def __init__ ( self , bucket : str , prefix : str , key : Optional [ str ] = None , local_path : Optional [ str ] = None , base_dir : str = \"/tmp\" , s3_client : Optional [ S3Boto3 ] = None , ** kwargs , ): \"\"\" Args: bucket: s3 bucket prefix: s3 prefix key: a key that represents the objects local_path: a specific local path to use. If None it is derived from prefix and base_dir base_dir: a base directory to use locally s3_client: the s3 client **kwargs: \"\"\" self . bucket = bucket self . prefix = prefix self . base_dir = base_dir self . key = key self . _name = None self . _ext = None self . _local_path = local_path self . s3_client = s3_client or S3Boto3 ()","title":"__init__()"},{"location":"references/MImage/#midatasets.mimage.S3Object.delete","text":"Delete an object locally Source code in midatasets/mimage.py 179 180 181 182 183 184 185 186 187 188 189 def delete ( self ): \"\"\" Delete an object locally Returns: \"\"\" try : os . remove ( self . local_path ) logger . info ( f \"[Removed] { self . local_path } \" ) except Exception as e : logger . warning ( e )","title":"delete()"},{"location":"references/MImage/#midatasets.mimage.S3Object.download","text":"Download an object from s3 Parameters: Name Type Description Default overwrite bool Whether to overwrite if it already exists False Source code in midatasets/mimage.py 138 139 140 141 142 143 144 145 146 147 148 149 150 def download ( self , overwrite : bool = False ): \"\"\" Download an object from s3 Args: overwrite: Whether to overwrite if it already exists Returns: None \"\"\" self . s3_client . download_file ( bucket = self . bucket , prefix = self . prefix , target = self . local_path , overwrite = overwrite )","title":"download()"},{"location":"references/MImage/#midatasets.mimage.S3Object.exists_local","text":"Check if object exists locally Source code in midatasets/mimage.py 163 164 165 166 167 168 169 def exists_local ( self ): \"\"\" Check if object exists locally Returns: True if exists \"\"\" return os . path . exists ( self . local_path )","title":"exists_local()"},{"location":"references/MImage/#midatasets.mimage.S3Object.exists_remote","text":"Check if object exists on s3 Source code in midatasets/mimage.py 171 172 173 174 175 176 177 def exists_remote ( self ): \"\"\" Check if object exists on s3 Returns: True if exists \"\"\" return self . s3_client . check_exists ( self . bucket , self . prefix )","title":"exists_remote()"},{"location":"references/MImage/#midatasets.mimage.S3Object.upload","text":"Upload an object to s3 Parameters: Name Type Description Default overwrite bool Whether to overwrite if it already exists False Source code in midatasets/mimage.py 152 153 154 155 156 157 158 159 160 161 def upload ( self , overwrite : bool = False ) -> None : \"\"\" Upload an object to s3 Args: overwrite: Whether to overwrite if it already exists Returns: None \"\"\" self . s3_client . upload_file ( file_name = self . local_path , bucket = self . bucket , prefix = self . prefix , overwrite = overwrite )","title":"upload()"},{"location":"references/dataset_clients/","text":"APIDatasetClient Bases: DatasetClientBase Use REST API as source of datasets Source code in midatasets/clients.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class APIDatasetClient ( DatasetClientBase ): \"\"\" Use REST API as source of datasets \"\"\" def __init__ ( self , host : Optional [ str ] = None , access_token : Optional [ str ] = None ): if access_token is None : logger . error ( \"Missing access token\" ) self . host = host self . access_token = access_token def _build_request ( self , path , method = \"GET\" , body = None , query = None ): headers = { \"Authorization\" : f \"Bearer { self . access_token } \" } url = urljoin ( self . host , path ) if query is not None : url += f \"? { urlencode ( query ) } \" if body : body = json . dumps ( body ) . encode () headers [ \"Content-Type\" ] = \"application/json\" return Request ( url , data = body , headers = headers , method = method ) def _send_request ( self , path , method = \"GET\" , body = None ): request = self . _build_request ( path , method , body ) with urlopen ( request , timeout = 120 ) as response : return response . read () @property def datasets_prefix ( self ): return \"/datasets\" def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets Returns: \"\"\" res = self . _send_request ( self . datasets_prefix ) return [ Dataset ( ** d ) for d in json . loads ( res . decode ( \"utf-8\" ))] def get_dataset ( self , id : int ) -> Dataset : \"\"\" Get dataset by id Args: id: Returns: \"\"\" res = self . _send_request ( f \" { self . datasets_prefix } / { id } \" ) return Dataset ( ** json . loads ( res . decode ( \"utf-8\" ))) def get_images ( self , dataset_id : int , skip = 0 , limit = 2000 ) -> List [ Image ]: \"\"\" Get images for dataset\"\"\" res = self . _send_request ( f \" { self . datasets_prefix } / { dataset_id } /images?skip= { skip } &limit= { limit } \" ) res = json . loads ( res . decode ( \"utf-8\" )) return [ Image ( ** d ) for d in res . get ( \"data\" , [])] get_dataset ( id ) Get dataset by id Parameters: Name Type Description Default id int required Source code in midatasets/clients.py 108 109 110 111 112 113 114 115 116 117 118 def get_dataset ( self , id : int ) -> Dataset : \"\"\" Get dataset by id Args: id: Returns: \"\"\" res = self . _send_request ( f \" { self . datasets_prefix } / { id } \" ) return Dataset ( ** json . loads ( res . decode ( \"utf-8\" ))) get_datasets () Get list of datasets Source code in midatasets/clients.py 99 100 101 102 103 104 105 106 def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets Returns: \"\"\" res = self . _send_request ( self . datasets_prefix ) return [ Dataset ( ** d ) for d in json . loads ( res . decode ( \"utf-8\" ))] get_images ( dataset_id , skip = 0 , limit = 2000 ) Get images for dataset Source code in midatasets/clients.py 120 121 122 123 124 125 126 127 128 def get_images ( self , dataset_id : int , skip = 0 , limit = 2000 ) -> List [ Image ]: \"\"\" Get images for dataset\"\"\" res = self . _send_request ( f \" { self . datasets_prefix } / { dataset_id } /images?skip= { skip } &limit= { limit } \" ) res = json . loads ( res . decode ( \"utf-8\" )) return [ Image ( ** d ) for d in res . get ( \"data\" , [])] DatasetClientBase Base class for dataset client Source code in midatasets/clients.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class DatasetClientBase : \"\"\" Base class for dataset client \"\"\" def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets Returns: \"\"\" raise NotImplementedError def get_dataset ( self , id : Union [ int , UUID , str ]) -> Dataset : \"\"\" Get dataset by id Args: id: dataset id Returns: \"\"\" raise NotImplementedError def get_images ( self , dataset_id : Union [ int , UUID , str ], skip : int = 0 , limit : int = 2000 ) -> List [ Image ]: \"\"\" Get images for dataset Args: dataset_id: dataset id skip: skip limit: limit Returns: \"\"\" raise NotImplementedError def _generate_uuid ( self , string : str ): \"\"\" Generate uuid from string Args: string: string to generate uuid from Returns: \"\"\" return ( uuid3 ( LOCAL_NAMESPACE , str ( string ))) get_dataset ( id ) Get dataset by id Parameters: Name Type Description Default id Union [ int , UUID , str ] dataset id required Source code in midatasets/clients.py 32 33 34 35 36 37 38 39 40 41 def get_dataset ( self , id : Union [ int , UUID , str ]) -> Dataset : \"\"\" Get dataset by id Args: id: dataset id Returns: \"\"\" raise NotImplementedError get_datasets () Get list of datasets Source code in midatasets/clients.py 24 25 26 27 28 29 30 def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets Returns: \"\"\" raise NotImplementedError get_images ( dataset_id , skip = 0 , limit = 2000 ) Get images for dataset Parameters: Name Type Description Default dataset_id Union [ int , UUID , str ] dataset id required skip int skip 0 limit int limit 2000 Source code in midatasets/clients.py 43 44 45 46 47 48 49 50 51 52 53 54 def get_images ( self , dataset_id : Union [ int , UUID , str ], skip : int = 0 , limit : int = 2000 ) -> List [ Image ]: \"\"\" Get images for dataset Args: dataset_id: dataset id skip: skip limit: limit Returns: \"\"\" raise NotImplementedError LocalDatasetClient Bases: DatasetClientBase Use local directory as source of datasets Examples: >>> client = LocalDatasetClient ( root_dir = \"/data\" ) >>> client . get_datasets () >>> client . get_dataset ( id = 1 ) >>> client . get_images ( dataset_id = 1 ) Source code in midatasets/clients.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 class LocalDatasetClient ( DatasetClientBase ): \"\"\" Use local directory as source of datasets Examples: >>> client = LocalDatasetClient(root_dir=\"/data\") >>> client.get_datasets() >>> client.get_dataset(id=1) >>> client.get_images(dataset_id=1) \"\"\" def __init__ ( self , root_dir : str ): self . root_dir = root_dir self . _datasets = None self . _images = None def get_datasets ( self ) -> List [ Dataset ]: datasets = [] for path in Path ( self . root_dir ) . iterdir (): datasets . append ( Dataset ( name = path . name , path = str ( path ), id = str ( path . name ))) self . _datasets = { str ( dataset . id ): dataset for dataset in datasets } return datasets @property def datasets_cached ( self ): if self . _datasets is None : self . get_datasets () return self . _datasets def get_dataset ( self , id : int ) -> Dataset : try : return self . datasets_cached [ id ] except : raise KeyError ( f \"Dataset { id } not found\" ) def _generate_uuid ( self , string : str ): return ( uuid3 ( LOCAL_NAMESPACE , str ( string ))) def get_images ( self , dataset_id : Union [ int , UUID , str ], skip = 0 , limit = 2000 ) -> List [ Image ]: dataset = self . get_dataset ( dataset_id ) backend = DatasetLocalBackend ( root_path = dataset . path ) files = backend . list_files ( grouped = True , spacing = 0 ) images = [] files = files . get ( \"native\" ) if files is None : return [] for name , file in files . items (): artifacts = [] for key , data in file . items (): artifacts . append ( Artifact ( key = key , path = data [ \"path\" ], id = self . _generate_uuid ( data [ \"path\" ]))) images . append ( Image ( name = name , id = self . _generate_uuid ( name ), artifacts = artifacts )) return images S3DatasetClient Bases: DatasetClientBase Use s3 as source of datasets Source code in midatasets/clients.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class S3DatasetClient ( DatasetClientBase ): \"\"\" Use s3 as source of datasets \"\"\" def __init__ ( self , bucket : str , prefix : Optional [ str ] = None ): self . bucket = bucket self . prefix = prefix self . _datasets = None self . _images = None def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets from s3 bucket Returns: \"\"\" backend = DatasetS3Backend ( bucket = self . bucket , prefix = self . prefix ) datasets = [] for name , prefix in backend . list_dirs () . items (): datasets . append ( Dataset ( name = name , path = f \"s3:// { self . bucket } / { prefix } \" , id = str ( name ))) self . _datasets = { str ( dataset . id ): dataset for dataset in datasets } return datasets @property def datasets_cached ( self ): if self . _datasets is None : self . get_datasets () return self . _datasets def get_dataset ( self , id : int ) -> Dataset : \"\"\" Get dataset by id Args: id: Returns: \"\"\" return self . datasets_cached [ id ] def get_images ( self , dataset_id : Union [ int , UUID , str ], skip = 0 , limit = 2000 ) -> List [ Image ]: \"\"\" Get images from dataset Args: dataset_id: skip: limit: Returns: \"\"\" dataset = self . get_dataset ( dataset_id ) backend = DatasetS3Backend ( bucket = self . bucket , prefix = dataset . path . split ( f \"s3:// { self . bucket } /\" )[ 1 ]) files = backend . list_files ( grouped = True , spacing = 0 ) images = [] files = files . get ( get_spacing_dirname ( 0 )) if files is None : return [] for name , file in files . items (): artifacts = [] for key , data in file . items (): artifacts . append ( Artifact ( key = key , path = data [ \"path\" ], id = self . _generate_uuid ( data [ \"path\" ]))) images . append ( Image ( name = name , id = self . _generate_uuid ( name ), artifacts = artifacts )) return images get_dataset ( id ) Get dataset by id Parameters: Name Type Description Default id int required Source code in midatasets/clients.py 213 214 215 216 217 218 219 220 221 222 def get_dataset ( self , id : int ) -> Dataset : \"\"\" Get dataset by id Args: id: Returns: \"\"\" return self . datasets_cached [ id ] get_datasets () Get list of datasets from s3 bucket Source code in midatasets/clients.py 195 196 197 198 199 200 201 202 203 204 205 206 def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets from s3 bucket Returns: \"\"\" backend = DatasetS3Backend ( bucket = self . bucket , prefix = self . prefix ) datasets = [] for name , prefix in backend . list_dirs () . items (): datasets . append ( Dataset ( name = name , path = f \"s3:// { self . bucket } / { prefix } \" , id = str ( name ))) self . _datasets = { str ( dataset . id ): dataset for dataset in datasets } return datasets get_images ( dataset_id , skip = 0 , limit = 2000 ) Get images from dataset Parameters: Name Type Description Default dataset_id Union [ int , UUID , str ] required skip 0 limit 2000 Source code in midatasets/clients.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def get_images ( self , dataset_id : Union [ int , UUID , str ], skip = 0 , limit = 2000 ) -> List [ Image ]: \"\"\" Get images from dataset Args: dataset_id: skip: limit: Returns: \"\"\" dataset = self . get_dataset ( dataset_id ) backend = DatasetS3Backend ( bucket = self . bucket , prefix = dataset . path . split ( f \"s3:// { self . bucket } /\" )[ 1 ]) files = backend . list_files ( grouped = True , spacing = 0 ) images = [] files = files . get ( get_spacing_dirname ( 0 )) if files is None : return [] for name , file in files . items (): artifacts = [] for key , data in file . items (): artifacts . append ( Artifact ( key = key , path = data [ \"path\" ], id = self . _generate_uuid ( data [ \"path\" ]))) images . append ( Image ( name = name , id = self . _generate_uuid ( name ), artifacts = artifacts )) return images","title":"Dataset clients"},{"location":"references/dataset_clients/#midatasets.clients.APIDatasetClient","text":"Bases: DatasetClientBase Use REST API as source of datasets Source code in midatasets/clients.py 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 class APIDatasetClient ( DatasetClientBase ): \"\"\" Use REST API as source of datasets \"\"\" def __init__ ( self , host : Optional [ str ] = None , access_token : Optional [ str ] = None ): if access_token is None : logger . error ( \"Missing access token\" ) self . host = host self . access_token = access_token def _build_request ( self , path , method = \"GET\" , body = None , query = None ): headers = { \"Authorization\" : f \"Bearer { self . access_token } \" } url = urljoin ( self . host , path ) if query is not None : url += f \"? { urlencode ( query ) } \" if body : body = json . dumps ( body ) . encode () headers [ \"Content-Type\" ] = \"application/json\" return Request ( url , data = body , headers = headers , method = method ) def _send_request ( self , path , method = \"GET\" , body = None ): request = self . _build_request ( path , method , body ) with urlopen ( request , timeout = 120 ) as response : return response . read () @property def datasets_prefix ( self ): return \"/datasets\" def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets Returns: \"\"\" res = self . _send_request ( self . datasets_prefix ) return [ Dataset ( ** d ) for d in json . loads ( res . decode ( \"utf-8\" ))] def get_dataset ( self , id : int ) -> Dataset : \"\"\" Get dataset by id Args: id: Returns: \"\"\" res = self . _send_request ( f \" { self . datasets_prefix } / { id } \" ) return Dataset ( ** json . loads ( res . decode ( \"utf-8\" ))) def get_images ( self , dataset_id : int , skip = 0 , limit = 2000 ) -> List [ Image ]: \"\"\" Get images for dataset\"\"\" res = self . _send_request ( f \" { self . datasets_prefix } / { dataset_id } /images?skip= { skip } &limit= { limit } \" ) res = json . loads ( res . decode ( \"utf-8\" )) return [ Image ( ** d ) for d in res . get ( \"data\" , [])]","title":"APIDatasetClient"},{"location":"references/dataset_clients/#midatasets.clients.APIDatasetClient.get_dataset","text":"Get dataset by id Parameters: Name Type Description Default id int required Source code in midatasets/clients.py 108 109 110 111 112 113 114 115 116 117 118 def get_dataset ( self , id : int ) -> Dataset : \"\"\" Get dataset by id Args: id: Returns: \"\"\" res = self . _send_request ( f \" { self . datasets_prefix } / { id } \" ) return Dataset ( ** json . loads ( res . decode ( \"utf-8\" )))","title":"get_dataset()"},{"location":"references/dataset_clients/#midatasets.clients.APIDatasetClient.get_datasets","text":"Get list of datasets Source code in midatasets/clients.py 99 100 101 102 103 104 105 106 def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets Returns: \"\"\" res = self . _send_request ( self . datasets_prefix ) return [ Dataset ( ** d ) for d in json . loads ( res . decode ( \"utf-8\" ))]","title":"get_datasets()"},{"location":"references/dataset_clients/#midatasets.clients.APIDatasetClient.get_images","text":"Get images for dataset Source code in midatasets/clients.py 120 121 122 123 124 125 126 127 128 def get_images ( self , dataset_id : int , skip = 0 , limit = 2000 ) -> List [ Image ]: \"\"\" Get images for dataset\"\"\" res = self . _send_request ( f \" { self . datasets_prefix } / { dataset_id } /images?skip= { skip } &limit= { limit } \" ) res = json . loads ( res . decode ( \"utf-8\" )) return [ Image ( ** d ) for d in res . get ( \"data\" , [])]","title":"get_images()"},{"location":"references/dataset_clients/#midatasets.clients.DatasetClientBase","text":"Base class for dataset client Source code in midatasets/clients.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 class DatasetClientBase : \"\"\" Base class for dataset client \"\"\" def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets Returns: \"\"\" raise NotImplementedError def get_dataset ( self , id : Union [ int , UUID , str ]) -> Dataset : \"\"\" Get dataset by id Args: id: dataset id Returns: \"\"\" raise NotImplementedError def get_images ( self , dataset_id : Union [ int , UUID , str ], skip : int = 0 , limit : int = 2000 ) -> List [ Image ]: \"\"\" Get images for dataset Args: dataset_id: dataset id skip: skip limit: limit Returns: \"\"\" raise NotImplementedError def _generate_uuid ( self , string : str ): \"\"\" Generate uuid from string Args: string: string to generate uuid from Returns: \"\"\" return ( uuid3 ( LOCAL_NAMESPACE , str ( string )))","title":"DatasetClientBase"},{"location":"references/dataset_clients/#midatasets.clients.DatasetClientBase.get_dataset","text":"Get dataset by id Parameters: Name Type Description Default id Union [ int , UUID , str ] dataset id required Source code in midatasets/clients.py 32 33 34 35 36 37 38 39 40 41 def get_dataset ( self , id : Union [ int , UUID , str ]) -> Dataset : \"\"\" Get dataset by id Args: id: dataset id Returns: \"\"\" raise NotImplementedError","title":"get_dataset()"},{"location":"references/dataset_clients/#midatasets.clients.DatasetClientBase.get_datasets","text":"Get list of datasets Source code in midatasets/clients.py 24 25 26 27 28 29 30 def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets Returns: \"\"\" raise NotImplementedError","title":"get_datasets()"},{"location":"references/dataset_clients/#midatasets.clients.DatasetClientBase.get_images","text":"Get images for dataset Parameters: Name Type Description Default dataset_id Union [ int , UUID , str ] dataset id required skip int skip 0 limit int limit 2000 Source code in midatasets/clients.py 43 44 45 46 47 48 49 50 51 52 53 54 def get_images ( self , dataset_id : Union [ int , UUID , str ], skip : int = 0 , limit : int = 2000 ) -> List [ Image ]: \"\"\" Get images for dataset Args: dataset_id: dataset id skip: skip limit: limit Returns: \"\"\" raise NotImplementedError","title":"get_images()"},{"location":"references/dataset_clients/#midatasets.clients.LocalDatasetClient","text":"Bases: DatasetClientBase Use local directory as source of datasets Examples: >>> client = LocalDatasetClient ( root_dir = \"/data\" ) >>> client . get_datasets () >>> client . get_dataset ( id = 1 ) >>> client . get_images ( dataset_id = 1 ) Source code in midatasets/clients.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 class LocalDatasetClient ( DatasetClientBase ): \"\"\" Use local directory as source of datasets Examples: >>> client = LocalDatasetClient(root_dir=\"/data\") >>> client.get_datasets() >>> client.get_dataset(id=1) >>> client.get_images(dataset_id=1) \"\"\" def __init__ ( self , root_dir : str ): self . root_dir = root_dir self . _datasets = None self . _images = None def get_datasets ( self ) -> List [ Dataset ]: datasets = [] for path in Path ( self . root_dir ) . iterdir (): datasets . append ( Dataset ( name = path . name , path = str ( path ), id = str ( path . name ))) self . _datasets = { str ( dataset . id ): dataset for dataset in datasets } return datasets @property def datasets_cached ( self ): if self . _datasets is None : self . get_datasets () return self . _datasets def get_dataset ( self , id : int ) -> Dataset : try : return self . datasets_cached [ id ] except : raise KeyError ( f \"Dataset { id } not found\" ) def _generate_uuid ( self , string : str ): return ( uuid3 ( LOCAL_NAMESPACE , str ( string ))) def get_images ( self , dataset_id : Union [ int , UUID , str ], skip = 0 , limit = 2000 ) -> List [ Image ]: dataset = self . get_dataset ( dataset_id ) backend = DatasetLocalBackend ( root_path = dataset . path ) files = backend . list_files ( grouped = True , spacing = 0 ) images = [] files = files . get ( \"native\" ) if files is None : return [] for name , file in files . items (): artifacts = [] for key , data in file . items (): artifacts . append ( Artifact ( key = key , path = data [ \"path\" ], id = self . _generate_uuid ( data [ \"path\" ]))) images . append ( Image ( name = name , id = self . _generate_uuid ( name ), artifacts = artifacts )) return images","title":"LocalDatasetClient"},{"location":"references/dataset_clients/#midatasets.clients.S3DatasetClient","text":"Bases: DatasetClientBase Use s3 as source of datasets Source code in midatasets/clients.py 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 class S3DatasetClient ( DatasetClientBase ): \"\"\" Use s3 as source of datasets \"\"\" def __init__ ( self , bucket : str , prefix : Optional [ str ] = None ): self . bucket = bucket self . prefix = prefix self . _datasets = None self . _images = None def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets from s3 bucket Returns: \"\"\" backend = DatasetS3Backend ( bucket = self . bucket , prefix = self . prefix ) datasets = [] for name , prefix in backend . list_dirs () . items (): datasets . append ( Dataset ( name = name , path = f \"s3:// { self . bucket } / { prefix } \" , id = str ( name ))) self . _datasets = { str ( dataset . id ): dataset for dataset in datasets } return datasets @property def datasets_cached ( self ): if self . _datasets is None : self . get_datasets () return self . _datasets def get_dataset ( self , id : int ) -> Dataset : \"\"\" Get dataset by id Args: id: Returns: \"\"\" return self . datasets_cached [ id ] def get_images ( self , dataset_id : Union [ int , UUID , str ], skip = 0 , limit = 2000 ) -> List [ Image ]: \"\"\" Get images from dataset Args: dataset_id: skip: limit: Returns: \"\"\" dataset = self . get_dataset ( dataset_id ) backend = DatasetS3Backend ( bucket = self . bucket , prefix = dataset . path . split ( f \"s3:// { self . bucket } /\" )[ 1 ]) files = backend . list_files ( grouped = True , spacing = 0 ) images = [] files = files . get ( get_spacing_dirname ( 0 )) if files is None : return [] for name , file in files . items (): artifacts = [] for key , data in file . items (): artifacts . append ( Artifact ( key = key , path = data [ \"path\" ], id = self . _generate_uuid ( data [ \"path\" ]))) images . append ( Image ( name = name , id = self . _generate_uuid ( name ), artifacts = artifacts )) return images","title":"S3DatasetClient"},{"location":"references/dataset_clients/#midatasets.clients.S3DatasetClient.get_dataset","text":"Get dataset by id Parameters: Name Type Description Default id int required Source code in midatasets/clients.py 213 214 215 216 217 218 219 220 221 222 def get_dataset ( self , id : int ) -> Dataset : \"\"\" Get dataset by id Args: id: Returns: \"\"\" return self . datasets_cached [ id ]","title":"get_dataset()"},{"location":"references/dataset_clients/#midatasets.clients.S3DatasetClient.get_datasets","text":"Get list of datasets from s3 bucket Source code in midatasets/clients.py 195 196 197 198 199 200 201 202 203 204 205 206 def get_datasets ( self ) -> List [ Dataset ]: \"\"\" Get list of datasets from s3 bucket Returns: \"\"\" backend = DatasetS3Backend ( bucket = self . bucket , prefix = self . prefix ) datasets = [] for name , prefix in backend . list_dirs () . items (): datasets . append ( Dataset ( name = name , path = f \"s3:// { self . bucket } / { prefix } \" , id = str ( name ))) self . _datasets = { str ( dataset . id ): dataset for dataset in datasets } return datasets","title":"get_datasets()"},{"location":"references/dataset_clients/#midatasets.clients.S3DatasetClient.get_images","text":"Get images from dataset Parameters: Name Type Description Default dataset_id Union [ int , UUID , str ] required skip 0 limit 2000 Source code in midatasets/clients.py 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def get_images ( self , dataset_id : Union [ int , UUID , str ], skip = 0 , limit = 2000 ) -> List [ Image ]: \"\"\" Get images from dataset Args: dataset_id: skip: limit: Returns: \"\"\" dataset = self . get_dataset ( dataset_id ) backend = DatasetS3Backend ( bucket = self . bucket , prefix = dataset . path . split ( f \"s3:// { self . bucket } /\" )[ 1 ]) files = backend . list_files ( grouped = True , spacing = 0 ) images = [] files = files . get ( get_spacing_dirname ( 0 )) if files is None : return [] for name , file in files . items (): artifacts = [] for key , data in file . items (): artifacts . append ( Artifact ( key = key , path = data [ \"path\" ], id = self . _generate_uuid ( data [ \"path\" ]))) images . append ( Image ( name = name , id = self . _generate_uuid ( name ), artifacts = artifacts )) return images","title":"get_images()"}]}